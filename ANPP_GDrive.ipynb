{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Group1_GDrive.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"RFYsG0pFf38a"},"source":["################################################################################\n","\"\"\"\n","This is a Programming Project for Affective, Social Neuroscience (SoSe 2020).\n","This module provides the predictive modeling for level of immersion based on\n"," valence and arousal using the dataset:\n","<https://box.fu-berlin.de/s/2bP2cdDaeefBy2n >\n","\n","Those are requirement to run this module:\n","    - python\n","    - keras\n","    - matplotlib\n","    - numpy\n","    - pandas\n","    - seaborn\n","    - statsmodels\n","    - sklearn\n","    - google\n","    - xgboost\n","\n","This modules consists of four main part:\n","    - PART I: Group and Data Selection\n","    - PART II: Data Preprocessing\n","    - PART III: Predictive Modeling\n","    - PART IV: Statistical Analysis with BIG FIVE\n","\n","We highly recommend to use the google colab using GPU.\n","\n","Authors: Andreea Al-Afuni, JiHoon Kim, Angela Sofia Royo Romero, and Bati Yilmaz\n","Last-modified : 5th October 2020\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fv8nQCLHQ8b5"},"source":["################################################################################# 0. Import the library\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","import statsmodels.formula.api as smf\n","from google.colab import drive                                                  \n","from keras.models import Model, Sequential\n","from keras import models\n","from keras import layers\n","from keras import Input\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras.utils import to_categorical\n","from keras.wrappers.scikit_learn import KerasRegressor\n","from sklearn import svm, linear_model, metrics\n","from sklearn import preprocessing\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score,mean_squared_error\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.svm import SVR\n","import xgboost as xgb\n","from xgboost.sklearn import XGBRegressor\n","sns.set(style=\"darkgrid\", color_codes=True)\n","np.random.seed(20201005)                                                        # To ensure the reproducibility of the programm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkFY7Kh8RhgU"},"source":["################################################################################ PART I: Group and Data Selection\n","                                                                                # 1. Load the dataset: SENT_GROUP_INFO and SENT_RATING_DATA\n","drive.mount('/gdrive', force_remount=True)                                      # Mount the files on the google drive to be used with google colab\n","\n","sg_index = \"B:E,AE:AI,CW:DJ,DM,FN\"                                              # Select the necessary dataset based on the column indices\n","\n","s_grp_dir = '/gdrive/My Drive/SCAN_seminar_data/SENT_GROUP_INFO.xlsx'           # Load the dataset SENT_GROUP_INFO \n","s_group = pd.read_excel(s_grp_dir, usecols = sg_index)                          # Only loading columns B to E (Case, Text, Condition, Language)\n","                                                                                #                      AE to AI (BFI scores)\n","                                                                                #                      CW to DJ (Reading experience ratings)\n","                                                                                #                      DM, FN (Attention check, Minus points for fast completion = DEG Time)\n","s_rat_dir = '/gdrive/My Drive/SCAN_seminar_data/SENT_RATING_DATA.xlsx'          # Load the dataset: SENT_RATING_DATA (all columns)\n","s_rating = pd.read_excel(s_rat_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCNlyljnRhkY"},"source":["################################################################################# 2. Group the Data: Coherent ENG+GER, HARRY AND PIPPI\n","\n","                                                                                # SENT_GROUP_INFO.xlsx\n","sg_harry = s_group['TEXT'] == 'HARRY'                                           # return true only for HARRY\n","sg_pippi = s_group['TEXT'] == 'PIPPI'                                           # return true only for PIPPI\n","sg_coherent = s_group['CONDITION'] == 'COHERENT'                                # return true only for COHERENT\n","sg_scrambled = s_group['CONDITION'] == 'SCRAMBLED'                              # return true only for SCRAMBLED\n","sg_eng = s_group['QESTN_LANGUAGE'] == 'ENG'                                     # return true only for ENG\n","sg_ger = s_group['QESTN_LANGUAGE'] == 'GER'                                     # return true only for GER\n","\n","                                                                                # SENT_RATING_DATA.xlsx\n","sr_harry = s_rating['TEXT'] == 'HARRY'                                          # retrun true only for HARRY\n","sr_pippi = s_rating['TEXT'] == 'PIPPI'                                          # retrun true only for PIPPI\n","sr_coherent = s_rating['CONDITION'] == 'COHERENT'                               # retrun true only for COHERENT\n","sr_scrambled = s_rating['CONDITION'] == 'SCRAMBLED'                             # retrun true only for SCRAMBLED\n","sr_eng = s_rating['LANGUAGE'] == 'ENG'                                          # retrun true only for ENG\n","sr_ger = s_rating['LANGUAGE'] == 'GER'                                          # retrun true only for GER\n","\n","                                                                                # Data filtering in the certain conditions\n","sg_co_harry = s_group[sg_harry & sg_coherent]                                   # Text: HARRY; Condition: COHERENT\n","sg_sc_harry = s_group[sg_harry & sg_scrambled]                                  # Text: HARRY; Condition: SCRAMBLED\n","sg_co_pippi = s_group[sg_pippi & sg_coherent]                                   # Text: PIPPI; Condition: COHERENT\n","sg_sc_pippi = s_group[sg_pippi & sg_scrambled]                                  # Text: PIPPI; Condition: SCRAMBLED\n","\n","categories = ['(29) SC_HARRY','(26) CO_HARRY','CO_PIPPI (21)','SC_PIPPI (21)']  # Plot a pie chart of the distribution of the whole dataset (N=97)\n","sizes = [len(sg_sc_harry),len(sg_co_harry),len(sg_sc_pippi),len(sg_sc_pippi)]\n","colors = ['#ff9999', '#ffc000', '#8fd9b6', '#d395d0']\n","wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 5}\n","\n","fig, ax = plt.subplots()\n","ax.pie(sizes,labels=categories,autopct='%1.1f%%',startangle=255,\n","       colors=colors,wedgeprops=wedgeprops)\n","plt.title('Dataset (N=97)')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hoPy8onDbWs"},"source":["################################################################################# 3. Select the Data (Exclude the \"bad data\") and visualisation\n","sg_co = s_group[sg_coherent]\n","sr_co = s_rating[sr_coherent]\n","                                                                                ### Selecting the cases with \"bad data\"\n","\n","                                                                                #Criteria that renders a case as \"bad data\" are:\n","b_DEG = sg_co.loc[sg_co['DEG_TIME']>100]['CASE']                                #       1. DEG_TIME > 100 (completion of the survey was too fast)\n","\n","b_ATT = sg_co.loc[sg_co['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS']>0]['CASE']      #       2. ATTENTION_CHECKS_COUNT_WRONG_ANSWERS > 0 (clicking random answeres)\n","\n","b_PAG = sr_co.loc[sr_co['PAGE_TIME']>999]['CASE']                               #       3. PAGE_TIME > 999 (pausing while completing the survey, can interfere with immersion)\n","\n","b_list = pd.concat([b_DEG, b_ATT, b_PAG], axis = 0)                             # Concatenate the \"bad data\" described above into a single list\n","\n","b_list = b_list.drop_duplicates()                                               # Delete the duplicates\n","b_case = list(b_list.values)                                                    # Make of list of the cases considered to be \"bad data\" (list of case number)\n","print('bad data case list:',b_case)\n","\n","                                                                                \n","                                                                                ### Excluding the \"bad data\" cases from the dataset\n","\n","a_group = sg_co.copy()                                                          # Copy the SENT_GROUP_INFO.coherent\n","a_rating = sr_co.copy()                                                         # Copy the # SENT_RATING_DATA.coherent\n","\n","for i in b_case:                                                                # Exclude the bad case from the coherent dataset\n","    a_rating = a_rating.drop(a_rating[a_rating['CASE'] == i].index)\n","    a_group = a_group.drop(a_group[a_group['CASE'] == i].index)\n","\n","sg_co_harry = a_group[sg_harry & sg_coherent]                                   # Text: HARRY; Condition: COHERENT, bad data excluded\n","sg_sc_harry = a_group[sg_harry & sg_scrambled]                                  # Text: HARRY; Condition: SCRAMBLED, bad data excluded\n","sg_co_pippi = a_group[sg_pippi & sg_coherent]                                   # Text: PIPPI; Condition: COHERENT, bad data excluded\n","sg_sc_pippi = a_group[sg_pippi & sg_scrambled]                                  # Text: PIPPI; Condition: SCRAMBLED, bad data excluded\n","sg_co_en_pippi = a_group[sg_pippi & sg_coherent & sg_eng]                       # Text: PIPPI; Condition: COHERENT; Language: ENG\n","sg_co_ge_pippi = a_group[sg_pippi & sg_coherent & sg_ger]                       # Text: PIPPI; Condition: COHERENT; Language: GER\n","\n","categories = ['CO_PIPPI (17)','CO_HARRY (24)']                                  # Plot the pie chart of the distribution of the COHERENT dataset (N=41)\n","                                                                                # Two pie chart slices:                                                                               \n","sizes = [len(sg_co_pippi),len(sg_co_harry)]                                     #    Text: PIPPI; Condition: COHERENT\n","colors = ['#ff9999', '#ffc000']                                                 #    Text: HARRY; Condition: COHERENT\n","wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 5}\n","\n","fig, ax = plt.subplots()\n","ax.pie(sizes,labels=categories,autopct='%1.1f%%',startangle=105,\n","       colors=colors,wedgeprops=wedgeprops)\n","plt.title('Coherent Dataset (N=41)')                                            \n","plt.show()                                                                     \n","\n","\n","categories = ['CO_ENG_PIPPI (12)','CO_GER_PIPPI (5)','(24) CO_HARRY']           # Plot the pie chart of the distribution of the COHERENT dataset (N=41)\n","sizes = [len(sg_co_en_pippi),len(sg_co_ge_pippi),len(sg_co_harry)]              # Three pie chart slices: \n","colors = ['#ff9999', '#ffc000','#d395d0']                                       #    Text: HARRY; Condition: COHERENT \n","wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 5}                     #    Text: PIPPI; Condition: COHERENT; Language: ENG\n","                                                                                #    Text: PIPPI; Condition: COHERENT; Language: GER\n","fig, ax = plt.subplots()\n","ax.pie(sizes,labels=categories,autopct='%1.1f%%',startangle=105,                # \n","       colors=colors,wedgeprops=wedgeprops)                                     # \n","plt.title('Selected Dataset (N=41)')                                            #\n","plt.show()                                                                      #"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFFQIPxdE80s"},"source":["################################################################################# 4. Set the data with Condition: COHERENT only\n","                                                                                # Loading data from SENT_RATING file for:\n","ar_co_harry = a_rating[sr_harry & sr_coherent]                                  #   Text: HARRY; Condition: COHERENT\n","ar_co_pippi = a_rating[sr_pippi & sr_coherent & sr_eng]                         #   Text: PIPPI; Condition: COHERENT; Language: ENG\n","\n","                                                                                #  Loading data from SENT_GROUP_INFO file for:\n","ag_co_harry = a_group[sg_harry & sg_coherent]                                   #   Text: HARRY; Condition: COHERENT\n","ag_co_pippi = a_group[sg_pippi & sg_coherent & sg_eng]                          #   Text: PIPPI; Condition: COHERENT; Language: ENG\n","\n","################################################################################# 5. Drop out the NaN column from the data from SENT_GROUP_INFO file\n","gh = ag_co_harry                                                                # Text: HARRY; Condition: COHERENT\n","gp = ag_co_pippi                                                                # Text: PIPPI; Condition: COHERENT; Language: ENG\n","                                                                                # Charachter names: legend\n","rh = ar_co_harry.loc[:,['CASE','AROUSAL_RATING','VALENCE_RATING']]              # Text: HARRY; Condition: COHERENT in SENT_RATING_DATA\n","                                                                                # rh = ratings harry from file SENT_RATING\n","                                                                                #      contains: CASE, AROUSAL ratings, VALENCE ratings for Text: HARRY; Condition: COHERENT\n","rp = ar_co_pippi.loc[:,['CASE','AROUSAL_RATING','VALENCE_RATING']]              # Text: PIPPI; Condition: COHERENT; Language: ENG in SENT_RAITING_DATA\n","rp = rp.dropna(axis=0)                                                          # drop out NaN row\n","                                                                                # rp = ratings pippi from file SENT_RATING\n","                                                                                #      contains: CASE, AROUSAL ratings, valence ratings for Text: PIPPI; Condition: COHERENT; Language: ENG\n","print('HARRY')\n","print(rh.isnull().sum(), 'NaN')                                                 # check the NaN\n","print('PIPPI')\n","print(rp.isnull().sum(), 'NaN')                                                 # check the NaN\n","\n","print(\"Coherent demographic informations: \")                                    # Demographic info on the countable columns\n","print(ag_co_harry.describe())\n","print(ar_co_harry.describe())\n","\n","plt.title('DEG_TIME and CASE (N=41)')                                           # Plot the DEG_TIME\n","plt.scatter(ag_co_harry['CASE'], ag_co_harry['DEG_TIME'], c='orange')\n","plt.scatter(ag_co_pippi['CASE'], ag_co_pippi['DEG_TIME'], c='purple')\n","plt.plot(sg_co['CASE'], sg_co['DEG_TIME'])\n","plt.show()\n","print('b_DEG', list(b_DEG))                                                     # Print the case of DEG_TIME\n","\n","plt.title('ATTENTION_COUNT_WRONG_ANSWERS and CASE (N=41)')                      # Plot the ATTENTION_COUNT_WRONG_ANSWERS\n","plt.scatter(ag_co_harry['CASE'],ag_co_harry['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS'],c='orange')\n","plt.scatter(ag_co_pippi['CASE'],ag_co_pippi['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS'],c='purple')\n","plt.plot(sg_co['CASE'],sg_co['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS'])\n","plt.show()\n","print('b_ATT', list(b_ATT))                                                     # Print the case of ATTENTION_CHECKS_COUNT_WRONG_ANSWERS\n","\n","plt.title('PAGE_TIME and CASE (N=41)')                                          # Plot the PAGE_TIME\n","plt.scatter(ar_co_harry['CASE'], ar_co_harry['PAGE_TIME'], c='orange')\n","plt.scatter(ar_co_pippi['CASE'], ar_co_pippi['PAGE_TIME'], c='purple')\n","plt.plot(sr_co['CASE'], sr_co['PAGE_TIME'])\n","plt.show()\n","print('b_PAG', list(b_PAG))                                                     # Print the case of PAGE_TIME"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjCHDZ5VP_OM"},"source":["s_rating_filtered = rh.filter([\"VALENCE_RATING\", \"AROUSAL_RATING\"])             # Plot the AROUSAL_RATING and VALENCE_RATING HARRY distribution\n","sns.distplot(s_rating_filtered[\"AROUSAL_RATING\"])                               # AROUSAL_RATING in Text: HARRY; Condition: COHERENT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xfv87HYCQm6a"},"source":["sns.distplot(s_rating_filtered['VALENCE_RATING'], color='orange')               # Plot the VALENCE_RATING in Text: HARRY; Condition: COHERENT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZEqGxfMQm3v"},"source":["sns.jointplot(x='AROUSAL_RATING',y='VALENCE_RATING',                            # Joint plot of the both VALENCE_RATING and AROUSAL_RATING in HARRY\n","              data = s_rating_filtered,kind='reg')                              # Text: HARRY; Condition: COHERENT"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIi2WgSaQyE4"},"source":["s_rating_filtered = rp.filter([\"VALENCE_RATING\", \"AROUSAL_RATING\"])             # Plot the AROUSAL_RATING and VALENCE_RATING PIPPI distribution\n","sns.distplot(s_rating_filtered[\"AROUSAL_RATING\"])                               # AROUSAL_RATING in Text: PIPPI; Condition: COHERENT; Language: ENG"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbh2-EOyQ1Pf"},"source":["sns.distplot(s_rating_filtered['VALENCE_RATING'], color='orange')               # Plot the VALENCEL_RATING in Text: PIPPI; Condition: COHERENT; Language: ENG"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8RnY1btQyCG"},"source":["sns.jointplot(x='AROUSAL_RATING',y='VALENCE_RATING',                            # Joint plot of the both VALENCE_RATING and AROUSAL_RATING in \n","              data = s_rating_filtered,kind='reg',color ='purple')              # Text: PIPPI; Condition: COHERENT; Language: ENG"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvgqFfTJAvU4"},"source":["################################################################################ PART II: Data Preprocessing\n","                                                                                # 6. Compound the reader_response for immersion\n","rg_set = pd.concat([gh,gp], axis = 0)                                           # Merge the HARRY and PIPPI dataset\n","a_group = rg_set.copy()                                                         # Copy the dataset\n","h_group = gh.copy()\n","p_group = gp.copy()\n","\n","## Characters names: legend:\n","# gh = data from SENT_GROUP_INFO Text: HARRY; Condition: COHERENT\n","#\n","# gp = data from SENT_GROUP_INFO Text: PIPPI; Condition: COHERENT; Language: ENG\n","#\n","# a_group: reading experience ratings for Text: HARRY; Condition: COHERENT and Text: PIPPI; Condition: COHERENT; Language: ENG\n","# h_group: reading experience ratings for Text: HARRY; Condition: COHERENT\n","# p_group: reading experience ratings for Text: PIPPI; Condition: COHERENT; Language: ENG\n","\n","a_group['IMMERSION'] = 0                                                        # Create the new column of 'IMMERSION'\n","\n","reader_response = [16,17]                                                       # Select the reader's responses\n","                                                                                # If you need it then check the columns order, \"print(a_group.columns)\"\n","for i in reader_response:                                                       # Sum the reader's response\n","    a_group['IMMERSION'] += a_group.iloc[:,i]\n","\n","a_group['IMMERSION']=a_group['IMMERSION']/len(reader_response)                  # Using the arithmetic mean of the reader's response\n","print(a_group.head(10))\n","sns.distplot(a_group[\"IMMERSION\"])                                   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPSMQFYdD5ib"},"source":["h_group['IMMERSION'] = 0                                                        # Create the new column of 'IMMERSION' for \n","                                                                                # Text: HARRY, Condition: COHERENT\n","reader_response = [8,9,10,11,15,16,21]                                          # Select the reader's responses\n","                                                                                # If you need it then check the columns order, \"print(a_group.columns)\"\n","for i in reader_response:                                                       # Sum the reader's response\n","    h_group['IMMERSION'] += h_group.iloc[:,i]\n","\n","h_group['IMMERSION']=h_group['IMMERSION']/len(reader_response)                  # Using the arithmetic mean of the reader's response\n","print(h_group.head(10))\n","sns.distplot(h_group[\"IMMERSION\"])                                              "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eunefYFbD-qG"},"source":["p_group['IMMERSION'] = 0                                                        # Create the new column of 'IMMERSION'\n","                                                                                # For Text: PIppi, Condition: COHERENT, Language: ENG\n","reader_response = [8,9,10,11,15,16,21]                                          # Select the reader's responses\n","                                                                                # If you need it then check the columns order, \"print(a_group.columns)\"\n","for i in reader_response:                                                       # Sum the reader's response\n","    p_group['IMMERSION'] += p_group.iloc[:,i]\n","\n","p_group['IMMERSION']=p_group['IMMERSION']/len(reader_response)                  # Using the arithmetic mean of the reader's response\n","print(p_group.head(10))\n","sns.distplot(p_group[\"IMMERSION\"])   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4Reqcycqmew"},"source":["################################################################################# 7. Preprocess the dataset\n","gh_case = list(a_group.loc[a_group['TEXT']=='HARRY'].loc[:,['CASE']]['CASE'])   # HARRY CASE LIST \"print(len(gh_case))\"\n","gp_case = list(a_group.loc[a_group['TEXT']=='PIPPI'].loc[:,['CASE']]['CASE'])   # PIPPI CASE LIST \"print(len(gp_case))\"\n","\n","tr_hr_case = gh_case[2:22]                                                      # training set for Text: HARRY, Condition: COHERENT\n","te_hr_case = gh_case[0:2] + gh_case[22:25]                                      # testing set for Text: HARRY, Condition: COHERENT\n","hr_case = [tr_hr_case, te_hr_case]                                              # training and testing cases for Text: HARRY, Condition: COHERENT\n","\n","tr_pi_case = gp_case[0:10]                                                      # training set for Text: PIPPI, Condition: COHERENT, Language = ENG\n","te_pi_case = gp_case[10:14]                                                     # testing set for Text: PIPPI, Condition: COHERENT, Language = ENG\n","pi_case = [tr_pi_case, te_pi_case]                                              # PIPPI case\n","\n","rh_set = rh.copy()                                                              # Load the independent variable related dataset\n","rp_set = rp.copy()\n","\n","hr_train_arousal = []\n","hr_train_valence = []\n","hr_test_arousal = []\n","hr_test_valence = []\n","\n","for i in hr_case:                                                               # Text: HARRY\n","    for j in i:                                                                 # Extract Arousal and Valence\n","        if i == tr_hr_case:\n","            set = rh_set[rh_set['CASE']==j]\n","            #new = set.sort_values(by='SENTENCE_NUMBER')\n","            hr_train_arousal.append(list(set.loc[:,['AROUSAL_RATING']]['AROUSAL_RATING']))\n","            hr_train_valence.append(list(set.loc[:,['VALENCE_RATING']]['VALENCE_RATING']))\n","        else:\n","            set = rh_set[rh_set['CASE']==j]\n","            #new = set.sort_values(by='SENTENCE_NUMBER')\n","            hr_test_arousal.append(list(set.loc[:,['AROUSAL_RATING']]['AROUSAL_RATING']))\n","            hr_test_valence.append(list(set.loc[:,['VALENCE_RATING']]['VALENCE_RATING']))\n","\n","pi_train_arousal = []\n","pi_train_valence = []\n","pi_test_arousal = []\n","pi_test_valence = []\n","\n","for i in pi_case:                                                               # Text: PIPPI\n","    for j in i:                                                                 # Extract Arousal and Valence\n","        if i == tr_pi_case:\n","            set = rp_set[rp_set['CASE']==j]\n","            #new = set.sort_values(by='SENTENCE_NUMBER')\n","            pi_train_arousal.append(list(set.loc[:,['AROUSAL_RATING']]['AROUSAL_RATING']))\n","            pi_train_valence.append(list(set.loc[:,['VALENCE_RATING']]['VALENCE_RATING']))\n","        else:\n","            set = rp_set[rp_set['CASE']==j]\n","            #new = set.sort_values(by='SENTENCE_NUMBER')\n","            pi_test_arousal.append(list(set.loc[:,['AROUSAL_RATING']]['AROUSAL_RATING']))\n","            pi_test_valence.append(list(set.loc[:,['VALENCE_RATING']]['VALENCE_RATING']))\n","\n","print('Coherent HARRY Dataset')                                                 # Set the input data for training set (as x_Train) and testing set (as x_Test)\n","print('train_arousal:',len(hr_train_arousal),hr_train_arousal)                  # The input data consists of the arounsal and valence ratings\n","print('train_valence:',len(hr_train_valence),hr_train_valence)\n","print('test_arousal:',len(hr_test_arousal),hr_test_arousal)\n","print('test_valence:',len(hr_test_valence),hr_test_valence)\n","\n","print('Coherent PIPPI Dataset')\n","print('train_arousal:',len(pi_train_arousal),pi_train_arousal)\n","print('train_valence:',len(pi_train_valence),pi_train_valence)\n","print('test_arousal:',len(pi_test_arousal),pi_test_arousal)\n","print('test_valence:',len(pi_test_valence),pi_test_valence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRWxQqTT-2Kx"},"source":["################################################################################ Set the output data for the training set (as Y_Train) and test set (as Y_Test)\n","hr_train_immersion = []                                                         # The output data consists of the Immersion level as calculated above\n","hr_test_immersion = []\n","\n","for i in hr_case:                                                               # HARRY IMMERSION DATASET\n","    for j in i:\n","        if i == tr_hr_case:\n","            hr_train_immersion.append(list(a_group[a_group['CASE']==j]['IMMERSION']))\n","        else:\n","            hr_test_immersion.append(list(a_group[a_group['CASE']==j]['IMMERSION']))\n","\n","print('hr_train_immersion',len(hr_train_immersion),hr_train_immersion)\n","print('hr_test_immersion',len(hr_test_immersion),hr_test_immersion)\n","\n","# PIPPI IMMERSION DATASET\n","pi_train_immersion = []\n","pi_test_immersion = []\n","\n","for i in pi_case:                                                               # PIPPI IMMERSION DATASET\n","    for j in i:\n","        if i == tr_pi_case:\n","            pi_train_immersion.append(list(a_group[a_group['CASE']==j]['IMMERSION']))\n","        else:\n","            pi_test_immersion.append(list(a_group[a_group['CASE']==j]['IMMERSION']))\n","\n","print('pi_train_immersion',len(pi_train_immersion),pi_train_immersion)\n","print('pi_test_immersion',len(pi_test_immersion),pi_test_immersion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aWH59hkySLn"},"source":["################################################################################ Dataset as np.array\n","print('HARRY Dataset')                                                          # HARRY DATASET\n","hr_tr_arousal = np.array(hr_train_arousal)\n","hr_tr_valence = np.array(hr_train_valence)\n","hr_tr_immersion = np.array(hr_train_immersion)\n","\n","hr_te_arousal = np.array(hr_test_arousal)\n","hr_te_valence = np.array(hr_test_valence)\n","hr_te_immersion = np.array(hr_test_immersion)\n","\n","print('HARRY Training set')                                                     \n","print('hr_tr_arousal',hr_tr_arousal.shape,hr_tr_arousal)\n","print('hr_tr_valence',hr_tr_valence.shape,hr_tr_valence)\n","print('hr_tr_immersion',hr_tr_immersion.shape,hr_tr_immersion)\n","\n","print('HARRY Test set')\n","print(hr_te_arousal)\n","print(hr_te_valence)\n","print(hr_te_immersion)\n","\n","print('PIPPI Dataset')                                                          # PIPPI DATASET\n","pi_tr_arousal = np.array(pi_train_arousal)\n","pi_tr_valence = np.array(pi_train_valence)\n","pi_tr_immersion = np.array(pi_train_immersion)\n","\n","pi_te_arousal = np.array(pi_test_arousal)\n","pi_te_valence = np.array(pi_test_valence)\n","pi_te_immersion = np.array(pi_test_immersion)\n","\n","print('PIPPI Training set')\n","print('pi_tr_arousal',pi_tr_arousal.shape,pi_tr_arousal)\n","print('pi_tr_valence',pi_tr_valence.shape,pi_tr_valence)\n","print('pi_tr_immersion',pi_tr_immersion.shape,pi_tr_immersion)\n","\n","print('PIPPI Test set')\n","print(pi_te_arousal)\n","print(pi_te_valence)\n","print(pi_te_immersion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzqYLjYfeRcS"},"source":["################################################################################# Concatenate to design flatten (1,) dataset\n","hr_tr_dataset = np.concatenate((hr_tr_arousal, hr_tr_valence), axis = 1)        #######   HARRY dataset\n","hr_te_dataset = np.concatenate((hr_te_arousal, hr_te_valence), axis = 1)        # In order to pass the input data in an optimal manner into the \n","print(hr_tr_dataset.shape,'tr_dataset')                                         # machine learning algorithm, we will create an input dataset in the shape of \n","print(hr_tr_dataset)                                                            # a two dimensional array (a, b) where\n","print(hr_te_dataset.shape,'te_dataset')                                         #   a=number of cases (2- fot the training set, 4 for the test set)\n","print(hr_te_dataset)                                                            #   b=250, in which the first 125 values represent the arousal scores \n","                                                                                #     and the next 125 values represent the the valence scores\n","\n","pi_tr_dataset = np.concatenate((pi_tr_arousal, pi_tr_valence), axis = 1)        #######    PIPPI dataset\n","pi_te_dataset = np.concatenate((pi_te_arousal, pi_te_valence), axis = 1)        # Same (a, b) structure as described above"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxXIpbI48qYx"},"source":["################################################################################ PART III: Predictive Modeling\n","def MAE(y_train, y_pred):                                                       # 8. Define Evaluation Score, Mean Absolute Error (MAE)\n","  return np.mean(np.abs((y_train - y_pred)))\n","\n","################################################################################# 9. Multiple Linear Regression for HARRY dataset (TEXT: Harry, CONDITION: COHERENT)\n","x_train = hr_tr_dataset.copy()\n","x_test = hr_te_dataset.copy()\n","y_train = hr_tr_immersion.copy()\n","y_test = hr_te_immersion.copy()\n","\n","mlr = LinearRegression()\n","mlr.fit(x_train, y_train) \n","\n","print(\"train score\")\n","y_pred = mlr.predict(x_train)\n","\n","for i, e in enumerate(y_pred):\n","    print(\"expected_value\",y_pred[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred))\n","\n","print(\"test score\")\n","y_pred0 = mlr.predict(x_test)\n","\n","for i, e in enumerate(y_pred0):\n","    print(\"expected_value\",y_pred0[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7c1utfl2tEv"},"source":["################################################################################# 10. k-Neighbors-Regression for HARRY (TEXT: HARRY, CONDITION: COHERENT)\n","x_train = hr_tr_dataset.copy()\n","x_test = hr_te_dataset.copy()\n","y_train = hr_tr_immersion.copy()\n","y_test = hr_te_immersion.copy()\n","\n","neigh = KNeighborsRegressor(n_neighbors = 5, weights = \"distance\")\n","neigh.fit(x_train, y_train) \n","\n","print(\"train score\")\n","y_pred1 = neigh.predict(x_train)\n","\n","for i, e in enumerate(y_pred1):\n","    print(\"expected_value\",y_pred1[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred1))\n","\n","print(\"test score\")\n","y_pred2 = neigh.predict(x_test)\n","\n","for i, e in enumerate(y_pred2):\n","    print(\"expected_value\",y_pred2[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipzKB4jkiwbP"},"source":["################################################################################# 11. SVR, Support Vector Regressoion for HARRY (TEXT: HARRY, CONDITION: COHERENT)\n","x_train = hr_tr_dataset.copy()\n","x_test = hr_te_dataset.copy()\n","y_train = hr_tr_immersion.copy()\n","y_test = hr_te_immersion.copy()\n","\n","SupportVectorRegModel = SVR()\n","SupportVectorRegModel.fit(x_train,y_train)\n","\n","print(\"train score\")\n","y_pred3 = SupportVectorRegModel.predict(x_train)\n","\n","for i, e in enumerate(y_pred3):\n","    print(\"expected_value\",y_pred3[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred3))\n","\n","print(\"test score\")\n","y_pred4 = SupportVectorRegModel.predict(x_test)\n","\n","for i, e in enumerate(y_pred4):\n","    print(\"expected_value\",y_pred4[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oeeNqtRiwu3"},"source":["################################################################################# 12. XGB Regression for HARRY (TEXT: HARRY, CONDITION: COHERENT)\n","x_train = hr_tr_dataset.copy()\n","x_test = hr_te_dataset.copy()\n","y_train = hr_tr_immersion.copy()\n","y_test = hr_te_immersion.copy()\n","\n","xgb1 = XGBRegressor()\n","parameters = {'nthread':[4],\n","              'objective':['reg:linear'],\n","              'learning_rate': [.03, 0.05, .07],\n","              'max_depth': [5, 6, 7],\n","              'min_child_weight': [4],\n","              'silent': [1],\n","              'subsample': [0.7],\n","              'colsample_bytree': [0.7],\n","              'n_estimators': [500]}\n","\n","xgb_grid = GridSearchCV(xgb1,parameters,cv = 2,n_jobs = 5,verbose=True)\n","xgb_grid.fit(x_train,y_train)\n","\n","print(xgb_grid.best_score_)\n","print(xgb_grid.best_params_)\n","\n","print(\"train score\")\n","y_pred5= xgb_grid.predict(x_train)\n","for i, e in enumerate(y_pred5):\n","    print(\"expected_value\",y_pred5[i], '\\t', \"real_value\", y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred5))\n","\n","print(\"test score\")\n","y_pred6 = xgb_grid.predict(x_test)\n","for i, e in enumerate(y_pred6):\n","    print(\"expected_value\",y_pred6[i], '\\t', \"real_value\", y_test[i])\n","\n","print('MAE',MAE(y_test, y_pred6))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rL7LDDubwsTy"},"source":["################################################################################# 13. NN Regression for HARRY (TEXT: HARRY, CONDITION: COHERENT)\n","x_train = hr_tr_dataset.copy()                                                  # Characteristics: 5 layers\n","x_test = hr_te_dataset.copy()                                                   # Layer 1: Input layer, size = x_train.shape[1] which is 250\n","y_train = hr_tr_immersion.copy()                                                # Layer 2, 3, 4: size = 64\n","y_test = hr_te_immersion.copy()                                                 # Layer 5: Output layer, size = 1\n","\n","model = models.Sequential()\n","model.add(layers.Dense(64, activation='relu',\n","                       input_shape=(x_train.shape[1],)))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(1))\n","model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","\n","history = model.fit(x_train, y_train,epochs = 100,batch_size=1, verbose=0)\n","\n","print(\"train score\")\n","test_mse_score, test_mae_score = model.evaluate(x_train, y_train)\n","result1 = model.predict(x_train, verbose=0)\n","for i, e in enumerate(result1):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_train[i])\n","\n","print(\"test score\")\n","test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n","\n","result = model.predict(x_test, verbose=0)\n","for i, e in enumerate(result):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_test[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llsiYeQHfoXD"},"source":["################################################################################# 14. Multiple Linear Regression for PIPPI (TEXT: PIPPI, CONDITION: COHERENT)\n","x_train = pi_tr_dataset.copy()\n","x_test = pi_te_dataset.copy()\n","y_train = pi_tr_immersion.copy()\n","y_test = pi_te_immersion.copy()\n","\n","mlr = LinearRegression()\n","mlr.fit(x_train, y_train) \n","\n","print(\"train score\")\n","y_pred7 = mlr.predict(x_train)\n","\n","for i, e in enumerate(y_pred7):\n","    print(\"expected_value\",y_pred7[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred7))\n","\n","print(\"test score\")\n","y_pred8 = mlr.predict(x_test)\n","\n","for i, e in enumerate(y_pred8):\n","    print(\"expected_value\",y_pred8[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7dGqYpAAfLZ"},"source":["################################################################################# 15. k-Neighbors-Regression for PIPPI (TEXT: PIPPI, CONDITION: COHERENT)\n","\n","x_train = pi_tr_dataset.copy()\n","x_test = pi_te_dataset.copy()\n","y_train = pi_tr_immersion.copy()\n","y_test = pi_te_immersion.copy()\n","\n","neigh = KNeighborsRegressor(n_neighbors = 5, weights = \"distance\")\n","neigh.fit(x_train, y_train) \n","\n","print(\"train score\")\n","y_pred9 = neigh.predict(x_train)\n","\n","for i, e in enumerate(y_pred9):\n","    print(\"expected_value\",y_pred9[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred9))\n","\n","print(\"test score\")\n","y_pred10 = neigh.predict(x_test)\n","\n","for i, e in enumerate(y_pred10):\n","    print(\"expected_value\",y_pred10[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EV1bfgT2uSqA"},"source":["################################################################################# 16. SVR, Support Vector Regression for PIPPI (TEXT: PIPPI, CONDITION: COHERENT)\n","x_train = pi_tr_dataset.copy()\n","x_test = pi_te_dataset.copy()\n","y_train = pi_tr_immersion.copy()\n","y_test = pi_te_immersion.copy()\n","\n","SupportVectorRegModel = SVR()\n","SupportVectorRegModel.fit(x_train,y_train)\n","\n","print(\"train score\")\n","y_pred11 = SupportVectorRegModel.predict(x_train)\n","\n","for i, e in enumerate(y_pred11):\n","    print(\"expected_value\",y_pred11[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_train, y_pred11))\n","\n","print(\"test score\")\n","y_pred12 = SupportVectorRegModel.predict(x_test)\n","\n","for i, e in enumerate(y_pred12):\n","    print(\"expected_value\",y_pred12[i],'\\t', \"real_value\",y_train[i])\n","\n","print('MAE:',MAE(y_test, y_pred12))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ysqie6BPuVju"},"source":["################################################################################# 17. XGB Regression for PIPPI (TEXT: PIPPI, CONDITION: COHERENT)\n","x_train = pi_tr_dataset.copy()\n","x_test = pi_te_dataset.copy()\n","y_train = pi_tr_immersion.copy()\n","y_test = pi_te_immersion.copy()\n","\n","xgb1 = XGBRegressor()\n","parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n","              'objective':['reg:linear'],\n","              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n","              'max_depth': [5, 6, 7],\n","              'min_child_weight': [4],\n","              'silent': [1],\n","              'subsample': [0.7],\n","              'colsample_bytree': [0.7],\n","              'n_estimators': [500]}\n","\n","xgb_grid = GridSearchCV(xgb1,parameters,cv = 2,n_jobs = 5,verbose=True)\n","xgb_grid.fit(x_train,y_train)\n","\n","print(\"train score\")\n","y_pred13= xgb_grid.predict(x_train)\n","for i, e in enumerate(y_pred13):\n","    print(\"expected_value\",y_pred13[i], '\\t', \"real_value\", y_train[i])\n","print('MAE',MAE(y_test, y_pred13))\n","\n","print(\"test score\")\n","y_pred14 = xgb_grid.predict(x_test)\n","for i, e in enumerate(y_pred14):\n","    print(\"expected_value\",y_pred14[i], '\\t', \"real_value\", y_test[i])\n","\n","print('MAE',MAE(y_test, y_pred14))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwmPl85Mubee"},"source":["################################################################################# 18. NN Regression for PIPPI (TEXT: PIPPI, CONDITION: COHERENT)\n","x_train = pi_tr_dataset.copy()                                                  # Characteristics of the neural network: see above (For Text: HARRY)\n","x_test = pi_te_dataset.copy()\n","y_train = pi_tr_immersion.copy()\n","y_test = pi_te_immersion.copy()\n","\n","model = models.Sequential()\n","model.add(layers.Dense(64, activation='relu',\n","                       input_shape=(x_train.shape[1],)))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(1))\n","model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","\n","history = model.fit(x_train, y_train,epochs = 100,batch_size=1, verbose=0)\n","\n","print(\"train score\")\n","test_mse_score, test_mae_score = model.evaluate(x_train, y_train)\n","result1 = model.predict(x_train, verbose=0)\n","for i, e in enumerate(result1):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_train[i])\n","\n","print(\"test score\")\n","test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n","\n","result = model.predict(x_test, verbose=0)\n","for i, e in enumerate(result):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_test[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpJtTTKnUucW"},"source":["################################################################################# 19. Neural network, Regression: Hyperparameter Optimization\n","x_train = hr_tr_dataset.copy()\n","x_test = hr_te_dataset.copy()\n","y_train = hr_tr_immersion.copy()\n","y_test = hr_te_immersion.copy()\n","\n","def gridsearch_model(neurons1, neurons2, neurons3):                             ######### Grid Search function\n","                                                                                ############  Input:\n","    model = models.Sequential()\n","    model.add(layers.Dense(neurons1, activation='relu',                         #   neurons1 : integer, first dropout layer parameter      \n","                       input_shape=(x_train.shape[1],)))\n","    model.add(Dense(neurons2,activation = 'relu'))                              #   neurons2 : integer, first dropout layer parameter\n","    model.add(Dense(neurons3,activation = 'relu'))                              #   neurons3 : integer, second dropout layer parameter\n","    model.add(Dense(1))\n","    \n","    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])             ###### Output:\n","    return model                                                                #    This function returns a model\n","\n","model = KerasRegressor(build_fn=gridsearch_model, nb_epoch=100,                 # Grid Search, Hyper-parameter tuning for Text: HARRY, Condition: COHERENT\n","                       batch_size=4, verbose=0)\n","\n","neurons1 = [32,64,128]                                                               \n","neurons2 = [32,64,128]\n","neurons3 = [32,64,128]\n","\n","param_grid = dict(neurons1 = neurons1,neurons2 = neurons2,neurons3=neurons3)    # Grid Search parameter on hyper parameters\n","grid = GridSearchCV(estimator = model, param_grid = param_grid,n_jobs=-1)       # GridSearchCV process constructs and evaluates a model \n","grid_result = grid.fit(x_train,y_train)                                         # for each combination of parameters.\n","\n","means = grid_result.cv_results_['mean_test_score']                              # Summarize results\n","stds = grid_result.cv_results_['std_test_score']\n","params = grid_result.cv_results_['params']\n","for mean,stdev,param in zip(means,stds,params):\n","    print(\"%f (%f) with: %r\" % (mean,stdev,param))\n","print(\"Best: %f using %s\" % (grid_result.best_score_,grid_result.best_params_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtdmXbreLKB0"},"source":["################################################################################# 20. Neural network, Regression: Cross-Validation\n","para = grid_result.best_params_                                                 # Cross Validation for Text: HARRY, Condition: COHERENT\n","append = []\n","a_list = ['neurons1','neurons2','neurons3']\n","for i in a_list:\n","    append.append(para[i])\n","print(append)\n","\n","k = 5                                                                           # K-fold cross-validation\n","num_val_samples = len(x_train) // k\n","num_epochs = 200\n","all_mae_scores = []\n","\n","def build_model():                                                              ####### Define the model\n","    model = models.Sequential()                                                 # No Input\n","    model.add(layers.Dense(append[0], activation='relu',                        # Output: this function returns a model\n","                           input_shape=(x_train.shape[1],)))\n","    model.add(layers.Dense(append[1], activation='relu'))\n","    model.add(layers.Dense(append[2], activation='relu'))\n","    model.add(layers.Dense(1))\n","    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","    return model\n","\n","for i in range(k):\n","    print('processing fold #:', i+1)\n","    val_data = x_train[i*num_val_samples: (i + 1)*num_val_samples]\n","    val_targets = y_train[i*num_val_samples: (i+1)*num_val_samples]\n","                                                                                # Build training and test sets\n","    partial_x_train = np.concatenate(                                           \n","        [x_train[:i * num_val_samples],\n","         x_train[(i+1) * num_val_samples:]],\n","         axis = 0)                                                              # partial_x_train: contains k-1 subgroups of the total train input dataset\n","    partial_train_targets = np.concatenate(\n","        [y_train[:i * num_val_samples],\n","         y_train[(i + 1) * num_val_samples:]],\n","         axis=0)                                                                # partial_train_targets: contains k-1 subgroups of the total train output data\n","\n","    model = build_model()\n","    history = model.fit(partial_x_train, partial_train_targets,\n","                        validation_data=(val_data, val_targets),\n","                        epochs = num_epochs, batch_size=4, verbose=0)\n","    mae_history = history.history['val_mae']\n","    all_mae_scores.append(mae_history)\n","\n","average_mae_history = [np.mean([x[i] for x in all_mae_scores]) for i in range(num_epochs)]\n","\n","plt.plot(range(1, len(average_mae_history)+1), average_mae_history)             # Validation MAE Visualization\n","plt.xlabel('Epochs')\n","plt.ylabel('Average Validation MAE')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEFCiIS7bD_C"},"source":["################################################################################# 21. Neural network, Regression: Evaluation\n","para = grid_result.best_params_                                                 # Final train and test of Text: HARRY, Condition: COHERENT\n","append = []\n","alist = ['neurons1','neurons2','neurons3']\n","for i in alist:\n","    print(para[i])\n","    append.append(para[i])\n","print(append)\n","\n","model = models.Sequential()\n","model.add(layers.Dense(append[0], activation='relu',\n","                       input_shape=(x_train.shape[1],)))\n","model.add(layers.Dense(append[1], activation='relu'))\n","model.add(layers.Dense(append[2], activation='relu'))\n","model.add(layers.Dense(1))\n","model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","\n","history = model.fit(x_train,y_train,epochs=115,batch_size=20,verbose=0)         # Please type the proper epochs\n","\n","print(\"train score\")\n","test_mse_score, test_mae_score = model.evaluate(x_train, y_train)\n","result1 = model.predict(x_train, verbose=0)\n","for i, e in enumerate(result1):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_train[i])\n","\n","print(\"test score\")\n","test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n","\n","result = model.predict(x_test, verbose=0)\n","for i, e in enumerate(result):\n","    print(\"expected_value\",sum(e,0.0)/len(e), '\\t', \"real_value\", y_test[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilxsnyMjRh4U"},"source":["################################################################################ PART IV: Statistical Analysis with BIG FIVE\n","################################################################################# 22. Statistical Analyses between Immersion and 5 BFI (TEXT: HARRY; PIPPI)\n","                                                                                # Using linear regression, ordinary least square\n","e_group = a_group.copy()                                                        # Text: HARRY; PIPPI, Condition: COHERENT, Immersion: 2 Reader_Response\n","e_group.head()\n","\n","bfi_list = []\n","for i in range(5):\n","    bfi_list.append(e_group.columns[i+4])\n","\n","def immersion_bfi_sa(bfi_list):                                                 # Function for the Statistical Analysis\n","    for i, bfi in enumerate(bfi_list):\n","        bfi_list = 'IMMERSION ~ '+ bfi\n","        res = smf.ols(formula =bfi_list, data = e_group).fit()\n","        print(bfi)\n","        print('R2: ', res.rsquared)\n","        print(res.summary())\n","        print('')\n","\n","def immersion_bfi_pl(bfi_list):                                                 # Function for the Plot the IMMERSION and BFI in linear regression\n","    color = ['red','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y=\"IMMERSION\", data = e_group, color = color[i])\n","\n","def immersion_bfi_pl2(bfi_list):                                                # Function for the Plot the IMMERSION and BFI in nonlinear regression\n","    color = ['red','orange','yellow','green','blue','indigo','purple',\n","             'red','orange','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y = \"IMMERSION\", data = e_group,\n","                    color = color[i], order = 2)\n","\n","immersion_bfi_sa(bfi_list)                                                      # Statistical Analysis\n","immersion_bfi_pl(bfi_list)                                                      # Plot the IMMERSION and BFI in linear regression\n","immersion_bfi_pl2(bfi_list)                                                     # Plot the IMMERSION and BFI in nonlinear regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VEJv72xp59U"},"source":["################################################################################# BFI_OPENNESS and IMMERSION distribution\n","x = e_group['BFI_OPENNESS']                                                     # BFI_OPENNESS and IMMERSION\n","y = e_group['IMMERSION']\n","plt.scatter(x,y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_Vwl7YsRh7n"},"source":["################################################################################ IMMERSION and BFI OPENNESS\n","res = smf.ols(formula='IMMERSION ~ BFI_OPENNESS', data = e_group).fit()         # Using linear regression, oridnary least square\n","sns.regplot(x=\"BFI_OPENNESS\", y=\"IMMERSION\", data=e_group, color='purple')\n","print('BFI_OPENESS')\n","print(res.summary())\n","\n","plt.figure(figsize=(10, 2))                                                     # Check the outlier lower -2, over 2 considered as outlier\n","plt.stem(res.resid_pearson)\n","plt.axhline(2, c=\"g\", ls=\"--\")\n","plt.axhline(-2, c=\"g\", ls=\"--\")\n","plt.title(\"\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8x_VG8JKgTfK"},"source":["################################################################################# BFI_EXTRAVERSION and IMMERSION\n","x = e_group['BFI_EXTRAVERSION']                                                 # BFI_EXTRAVERSION and IMMERSION distribution\n","y = e_group['IMMERSION']\n","plt.scatter(x,y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrR1_VgXgTeC"},"source":["################################################################################# IMMERSION AND BFI_EXTRAVERSION\n","                                                                                # Using linear regression, oridnary least square\n","res = smf.ols(formula='IMMERSION ~ BFI_EXTRAVERSION', data = e_group).fit()\n","sns.regplot(x=\"BFI_EXTRAVERSION\", y=\"IMMERSION\", data=e_group, color='purple')\n","print('BFI_EXTRAVERSION')\n","print(res.summary())\n","\n","plt.figure(figsize=(10, 2))                                                     # Check the outlier lower -2, over 2 considered as outlier\n","plt.stem(res.resid_pearson)\n","plt.axhline(2, c=\"g\", ls=\"--\")\n","plt.axhline(-2, c=\"g\", ls=\"--\")\n","plt.title(\"\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OUAi7Xn2LZU"},"source":["################################################################################# 23. Statistical Analyses between Immersion and 5 BFI (TEXT: HARRY)\n","                                                                                # Using linear regression, ordinary least square\n","e_group = h_group.copy()                                                        # Text: HARRY, Condition: COHERENT, Immersion: 2 Reader_Response\n","e_group.head()\n","\n","bfi_list = []\n","for i in range(5):\n","    bfi_list.append(e_group.columns[i+4])\n","\n","def immersion_bfi_sa(bfi_list):                                                 # Function for the Statistical Analysis\n","    for i, bfi in enumerate(bfi_list):\n","        bfi_list = 'IMMERSION ~ '+ bfi\n","        res = smf.ols(formula =bfi_list, data = e_group).fit()\n","        print(bfi)\n","        print('R2: ', res.rsquared)\n","        print(res.summary())\n","        print('')\n","\n","def immersion_bfi_pl(bfi_list):                                                 # Function for the Plot the IMMERSION and BFI in linear regression\n","    color = ['red','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y=\"IMMERSION\", data = e_group, color = color[i])\n","\n","def immersion_bfi_pl2(bfi_list):                                                # Function for the Plot the IMMERSION and BFI in nonlinear regression\n","    color = ['red','orange','yellow','green','blue','indigo','purple',\n","             'red','orange','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y = \"IMMERSION\", data = e_group,\n","                    color = color[i], order = 2)\n","\n","immersion_bfi_sa(bfi_list)                                                      # Statistical Analysis\n","immersion_bfi_pl(bfi_list)                                                      # Plot the IMMERSION and BFI in linear regression\n","immersion_bfi_pl2(bfi_list)                                                     # Plot the IMMERSION and BFI in nonlinear regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"czCwqmUw2Lg8"},"source":["################################################################################# 24. Statistical Analyses between Immersion and 5 BFI (TEXT: PIPPI)\n","                                                                                # Using linear regression, ordinary least square\n","e_group = p_group.copy()                                                        # Text: PIPPI, Condition: COHERENT, Immersion: 3 Reader_Response\n","e_group.head()\n","\n","bfi_list = []\n","for i in range(5):\n","    bfi_list.append(e_group.columns[i+4])\n","\n","def immersion_bfi_sa(bfi_list):                                                 # Function for the Statistical Analysis\n","    for i, bfi in enumerate(bfi_list):\n","        bfi_list = 'IMMERSION ~ '+ bfi\n","        res = smf.ols(formula =bfi_list, data = e_group).fit()\n","        print(bfi)\n","        print('R2: ', res.rsquared)\n","        print(res.summary())\n","        print('')\n","\n","def immersion_bfi_pl(bfi_list):                                                 # Function for the Plot the IMMERSION and BFI in linear regression\n","    color = ['red','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y=\"IMMERSION\", data = e_group, color = color[i])\n","\n","def immersion_bfi_pl2(bfi_list):                                                # Function for the Plot the IMMERSION and BFI in nonlinear regression\n","    color = ['red','orange','yellow','green','blue','indigo','purple',\n","             'red','orange','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y = \"IMMERSION\", data = e_group,\n","                    color = color[i], order = 2)\n","\n","immersion_bfi_sa(bfi_list)                                                      # Statistical Analysis\n","immersion_bfi_pl(bfi_list)                                                      # Plot the IMMERSION and BFI in linear regression\n","immersion_bfi_pl2(bfi_list)                                                     # Plot the IMMERSION and BFI in nonlinear regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnR5Xs7xut-_"},"source":["################################################################################# Statistical Analysises between Immersion and 5 BFI: Artihmetic Mean\n","                                                                                # 25. Immersion and BIG_FIVE: All Reader_response (CONDITION: COHERENT; SCRAMBLED)\n","e_group = p_group.copy()                                                        # Text: HARRY; PIPPI, Condition: COHERENT; SCRAMBLED\n","e_group.head()\n","\n","b_DEG = s_group.loc[s_group['DEG_TIME']>100]['CASE']\n","b_ATT = s_group.loc[s_group['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS']>0]['CASE']\n","b_PAG = s_rating.loc[s_rating['PAGE_TIME']>999]['CASE']\n","b_list = pd.concat([b_DEG, b_ATT, b_PAG], axis = 0)\n","b_list = b_list.drop_duplicates()\n","b_case = list(b_list.values)\n","print(b_case)\n","\n","e_group = []\n","e_rating = []\n","a_group = s_group.copy()\n","a_rating = s_rating.copy()\n","\n","for i in b_case:\n","    a_rating = a_rating.drop(a_rating[a_rating['CASE'] == i].index)\n","    a_group = a_group.drop(a_group[a_group['CASE'] == i].index)\n","    if i == 46:\n","        e_group = a_group\n","        e_rating = a_rating\n","\n","e_group['IMMERSION'] = 0\n","\n","for i in range (14):\n","    e_group['IMMERSION'] += e_group.iloc[:,i+4]\n","\n","e_group['IMMERSION']=e_group['IMMERSION']/14\n","print(e_group.head(10))\n","sns.distplot(e_group[\"IMMERSION\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WlG9EcWwwgIA"},"source":["################################################################################# Correlation between BIG FIVE and IMMERSION \n","bfi_list = []\n","for i in range(5):\n","  bfi_list.append(e_group.columns[i+4])\n","\n","immersion_bfi_sa(bfi_list)\n","immersion_bfi_pl(bfi_list)\n","immersion_bfi_pl2(bfi_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5q_hYkNxSIF"},"source":["################################################################################ IMMERSION and BFI OPENNESS\n","res = smf.ols(formula='IMMERSION ~ BFI_CONSCIENTIOUSNESS', data = e_group).fit()         # Using linear regression, oridnary least square\n","sns.regplot(x=\"BFI_CONSCIENTIOUSNESS\", y=\"IMMERSION\", data=e_group, color='purple')\n","print('BFI_OPENESS')\n","print(res.summary())\n","\n","plt.figure(figsize=(10, 2))                                                     # Check the outlier lower -2, over 2 considered as outlier\n","plt.stem(res.resid_pearson)\n","plt.axhline(2, c=\"g\", ls=\"--\")\n","plt.axhline(-2, c=\"g\", ls=\"--\")\n","plt.title(\"\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnFMhb9ZxSWf"},"source":["################################################################################ IMMERSION and BFI OPENNESS\n","res = smf.ols(formula='IMMERSION ~ BFI_OPENNESS', data = e_group).fit()         # Using linear regression, oridnary least square\n","sns.regplot(x=\"BFI_OPENNESS\", y=\"IMMERSION\", data=e_group, color='purple')\n","print('BFI_OPENESS')\n","print(res.summary())\n","\n","plt.figure(figsize=(10, 2))                                                     # Check the outlier lower -2, over 2 considered as outlier\n","plt.stem(res.resid_pearson)\n","plt.axhline(2, c=\"g\", ls=\"--\")\n","plt.axhline(-2, c=\"g\", ls=\"--\")\n","plt.title(\"\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4a-zV7QbvAFw"},"source":["################################################################################# Statistical Analysises between Immersion and 5 BFI: Artihmetic Mean\n","                                                                                # 26. Immersion and BIG_FIVE: All Reader_response (CONDITION: COHERENT)\n","sg_co = s_group[sg_coherent]                                                    # Text: HARRY; PIPPI, Condition: COHERENT\n","sr_co = s_rating[sr_coherent]\n","                                                                                ### Selecting the cases with \"bad data\"\n","\n","                                                                                # Criteria that renders a case as \"bad data\" are:\n","b_DEG = sg_co.loc[sg_co['DEG_TIME']>100]['CASE']                                #       1. DEG_TIME > 100 (completion of the survey was too fast)\n","\n","b_ATT = sg_co.loc[sg_co['ATTENTION_CHECKS_COUNT_WRONG_ANSWERS']>0]['CASE']      #       2. ATTENTION_CHECKS_COUNT_WRONG_ANSWERS > 0 (clicking random answeres)\n","\n","b_PAG = sr_co.loc[sr_co['PAGE_TIME']>999]['CASE']                               #       3. PAGE_TIME > 999 (pausing while completing the survey, can interfere with immersion)\n","\n","b_list = pd.concat([b_DEG, b_ATT, b_PAG], axis = 0)                             # Concatenate the \"bad data\" described above into a single list\n","\n","b_list = b_list.drop_duplicates()                                               # Delete the duplicates\n","b_case = list(b_list.values)                                                    # Make of list of the cases considered to be \"bad data\" (list of case number)\n","print('bad data case list:',b_case)\n","                                                                                \n","                                                                                ### Excluding the \"bad data\" cases from the dataset\n","\n","a_group = sg_co.copy()                                                          # Copy the SENT_GROUP_INFO.coherent\n","a_rating = sr_co.copy()                                                         # Copy the # SENT_RATING_DATA.coherent\n","\n","for i in b_case:                                                                # Exclude the bad case from the coherent dataset\n","    a_rating = a_rating.drop(a_rating[a_rating['CASE'] == i].index)\n","    a_group = a_group.drop(a_group[a_group['CASE'] == i].index)\n","\n","sg_co_harry = a_group[sg_harry & sg_coherent]                                   # Text: HARRY; Condition: COHERENT, bad data excluded\n","sg_sc_harry = a_group[sg_harry & sg_scrambled]                                  # Text: HARRY; Condition: SCRAMBLED, bad data excluded\n","sg_co_pippi = a_group[sg_pippi & sg_coherent]                                   # Text: PIPPI; Condition: COHERENT, bad data excluded\n","sg_sc_pippi = a_group[sg_pippi & sg_scrambled]                                  # Text: PIPPI; Condition: SCRAMBLED, bad data excluded\n","sg_co_en_pippi = a_group[sg_pippi & sg_coherent & sg_eng]                       # Text: PIPPI; Condition: COHERENT; Language: ENG\n","sg_co_ge_pippi = a_group[sg_pippi & sg_coherent & sg_ger]                       # Text: PIPPI; Condition: COHERENT; Language: GER\n","\n","                                                                                #  Loading data from SENT_GROUP_INFO file for:\n","ag_co_harry = a_group[sg_harry & sg_coherent]                                   #   Text: HARRY; Condition: COHERENT\n","ag_co_pippi = a_group[sg_pippi & sg_coherent & sg_eng]                          #   Text: PIPPI; Condition: COHERENT; Language: ENG\n","                                                                                # drop out the NaN column from the data from SENT_GROUP_INFO file:\n","gh = ag_co_harry                                                                # Text: HARRY; Condition: COHERENT\n","gp = ag_co_pippi                                                                # Text: PIPPI; Condition: COHERENT; Language: ENG"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKTW9yhpw4Lx"},"source":["rg_set = pd.concat([gh,gp], axis = 0)                                           # Merge the HARRY and PIPPI dataset\n","a_group = rg_set.copy()                                                         # Copy the dataset\n","h_group = gh.copy()\n","p_group = gp.copy()\n","\n","a_group['IMMERSION'] = 0                                                        # Create the new column of 'IMMERSION'\n","\n","reader_response = [9,10,11,12,13,14,15,16,17,18,19,20,21,22]                    # Use all reader's response\n","                                                                                # If you need it then check the columns order, \"print(a_group.columns)\"\n","for i in reader_response:                                                       # Sum the reader's response\n","    a_group['IMMERSION'] += a_group.iloc[:,i]\n","\n","a_group['IMMERSION']=a_group['IMMERSION']/len(reader_response)                  # Using the arithmetic mean of the reader's response\n","print(a_group.head(10))\n","sns.distplot(a_group[\"IMMERSION\"])                          "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09ZV5t-XxOjc"},"source":["################################################################################# Statistical Analysises between Immersion and 5 BFI: Mean case\n","                                                                                # Using linear regression, ordinary least square\n","e_group = a_group.copy()\n","e_group.head()\n","\n","bfi_list = []\n","for i in range(5):\n","    bfi_list.append(e_group.columns[i+4])\n","\n","def immersion_bfi_sa(bfi_list):                                                 # Function for the Statistical Analysis\n","    for i, bfi in enumerate(bfi_list):\n","        bfi_list = 'IMMERSION ~ '+ bfi\n","        res = smf.ols(formula =bfi_list, data = e_group).fit()\n","        print(bfi)\n","        print('R2: ', res.rsquared)\n","        print(res.summary())\n","        print('')\n","\n","def immersion_bfi_pl(bfi_list):                                                 # Function for the Plot the IMMERSION and BFI in linear regression\n","    color = ['red','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y=\"IMMERSION\", data = e_group, color = color[i])\n","\n","def immersion_bfi_pl2(bfi_list):                                                # Function for the Plot the IMMERSION and BFI in nonlinear regression\n","    color = ['red','orange','yellow','green','blue','indigo','purple',\n","             'red','orange','yellow','green','blue','indigo','purple']\n","    for i, bfi in enumerate(bfi_list):    \n","        sns.regplot(x = bfi, y = \"IMMERSION\", data = e_group,\n","                    color = color[i], order = 2)\n","\n","immersion_bfi_sa(bfi_list)                                                      # Statistical Analysis\n","immersion_bfi_pl(bfi_list)                                                      # Plot the IMMERSION and BFI in linear regression\n","immersion_bfi_pl2(bfi_list)                                                     # Plot the IMMERSION and BFI in nonlinear regression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9MCrQUtxTu7"},"source":["################################################################################ IMMERSION and BFI OPENNESS\n","res = smf.ols(formula='IMMERSION ~ BFI_EXTRAVERSION', data = e_group).fit()         # Using linear regression, oridnary least square\n","sns.regplot(x=\"BFI_EXTRAVERSION\", y=\"IMMERSION\", data=e_group, color='purple')\n","print('BFI_OPENESS')\n","print(res.summary())\n","\n","plt.figure(figsize=(10, 2))                                                     # Check the outlier lower -2, over 2 considered as outlier\n","plt.stem(res.resid_pearson)\n","plt.axhline(2, c=\"g\", ls=\"--\")\n","plt.axhline(-2, c=\"g\", ls=\"--\")\n","plt.title(\"\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lffQ2n5AZXaY"},"source":["################################################################################# 27. Reader Response and BIG FIVE (TEXT: HARRY; PIPPI, CONDITION:COHERENT)\n","for reader in ['FOCUSING_OF_ATTENTION','TEXT_ABSORPTION','IMAGINABILITY',\n","               'SPATIAL_INVOLVEMENT','GENERAL_READING_ENJOYMENT',\n","               'IDENTIFICATION','EASE_OF_COGNITIVE_ACCESS']:\n","    e_group = a_group.copy()\n","    e_group.head()\n","\n","    bfi_list = []\n","\n","    for i in range(5):\n","        bfi_list.append(e_group.columns[i+4])\n","\n","    def reader_bfi_sa(reader, bfi_list):                                        # Function for the Statistical Analysis\n","        for i, bfi in enumerate(bfi_list):\n","            bfi_list = reader +'~ '+ bfi\n","            res = smf.ols(formula =bfi_list, data = e_group).fit()\n","            print(bfi)\n","            print('R2: ', res.rsquared)\n","            print(res.summary())\n","            print('')\n","\n","    reader_bfi_sa(reader, bfi_list)    "],"execution_count":null,"outputs":[]}]}